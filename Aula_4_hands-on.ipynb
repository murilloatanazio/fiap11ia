{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# demais pacotes serão importados, mas vamos adicioná-los à medida que houver necessidade "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adquirindo os dados\n",
    "\n",
    "Vamos fazer a leitura de nossos dados utilizando Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('bases/forestfires.csv',sep=';')\n",
    "\n",
    "# Imprimindo as primeiras 5 linhas\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Diretamente, não temos informação do que se refere cada coluna de nosso dataset (a não ser as óbvias month e day, etc.). Por isso, temos um arquivo extra que nos informa o significado de cada coluna. Deixei ele junto com o dataset no portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos usar o método info() que é útil para obter uma rápida descrição dos dados, em particular o número de linhas, o tipo de cada \n",
    "# atributo e o número de valores não nulos\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Todos os atributos são ou int ou float, exceto month e day, que é object, ou seja, pode armazenar qualquer tipo de objeto. Podemos inspecionar esses atributos em mais detalhes usando o método value_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'].value_counts()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Essa informação é interessante. Como o parque fica localizado em Portugal, agosto e setembro são meses de calor, o que pode contribuir para queimadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método describe() apresenta um resumo dos atributos numéricos\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "As linhas count, mean, min e max são auto-explicativas. std mostra o desvio padrão (mede o quão disperso os valores são); 25%, 50% e 75% \n",
    "correspondem a percentis. Um percentil indica um valor abaixo do qual fica uma determinada porcentagem de observações de um grupo qualquer.\n",
    "\n",
    "Várias informações úteis nos são apresentadas quando utilizamos o método describe, mas as informações da coluna rain nos chamam a atenção. Podemos ver que em 75% dos casos não há chuvas, o que contribui para a expansão das queimadas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de apoio\n",
    "# Não se preocupe se não entender como funciona o plot de gráficos agora. Vamos estudar isso em detalhes na próxima aula\n",
    "# deixei aqui apenas para detalhar um pouco mais nosso trabalho\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "data.hist(bins=50, figsize=(20,15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlação\n",
    "\n",
    "Podemos calcular o coeficiente de correlação padrão (Correlação de Pearson)entre cada par de atributos usando o método corr(). Vamos ver o quanto cada atributo se relaciona com area (aquilo que estamos tentando predizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = data.corr()\n",
    "corr_matrix[\"area\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Muita coisa interessante a ser observada, mas o atributo rain novamente nos chama a atenção. Podemos confirmar nossa suposição anterior ao verificar que, quanto mais chuva, menos área queimada, e vice versa, já que a correlação é negativa entre esses dois atributos. Podemos afirmar também que quanto maior a temperatura, maior a área queimada. Entetando, os valores de correlação são muito baixos e parecem não explicar totalmente a causa das queimadas (o próprio artigo deixa claro que o problema é muito difícil). \n",
    "\n",
    "Há algumas razões:\n",
    "\n",
    "1) O dataset pode não ser suficientemente grande\n",
    "2) o dataset pode não ser bom\n",
    "3) faltam mais atributos para determinar o output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando os Dados para Algoritmos de Machine Learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Poderíamos preparar os dados de modo manual, mas é mais interessante fazer isso escrevendo funções, por várias razões:\n",
    "    -> permite reproduzir essas transformações facilmente em qualquer dataset\n",
    "    -> gradualmente, você estará construindo sua biblioteca de funções de transformação que poderá ser reusada em projetos futuros.\n",
    "    -> poderemos usar essas funções em seus sistemas em produção para transformar novos dados antes de inseri-los em seu algoritmo\n",
    "    -> possibilidade de testar varias transformações e ver qual combinação funciona melhor"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Primeiro, precisamos voltar para o dataset original e vamos separar os preditores dos rótulos, visto que não queremos necessariamente aplicar a mesma transformação nos preditores e nos rótulos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop('area', axis=1)\n",
    "labels = data['area'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpeza dos Dados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A maioria dos algoritmos de machine learning não funciona com características faltantes, então precisamos criar algumas funções para lidar com elas. \n",
    "\n",
    "Olhando nosso dataset, podemos observar alguns valores faltantes para o atributo temp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Para arrumar essa inconsistência, temos três opções:\n",
    "    -> eleminar as amostras correspondentes\n",
    "    -> eliminar todo o atributo\n",
    "    -> ajustar os dados para algum valor (zero, média, mediana, etc.)\n",
    "    \n",
    "Podemos usar alguns métodos do Pandas para isso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.dropna(subset=[\"temp\"]) # opção 1\n",
    "# data.drop(\"temp\", axis=1) # opção 2\n",
    "# median = data[\"temp\"].median()\n",
    "# data[\"temp\"].fillna(median) # opção 3\n",
    "\n",
    "# lembrando que se a opção 3 for escolhida, precisamos tratar os dados tanto no conjunto de treino quanto no conjunto de teste"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Scikit_learn provê uma classe para cuidar dos valores faltantes: Imputer. \n",
    "\n",
    "Primeiro, precisamos criar uma instância dessa classe, especificando que desejamos substituir cada valor faltante em cada atributo pela mediana desse atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Visto que a mediana só pode ser calculada em atributos numéricos, precisamos criar uma cópia dos dados sem os atributos do tipo objeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = data.drop(['month','day'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora precisamos ajustar a instância do Imputer aos dados de treinamento usando o método fit():\n",
    "imputer.fit(data_num)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "o imputer calculou a mediana de cada atributo e armazenou o resultado na variável statistics_. Originalmente, apenas o atributo temp possuia valores faltantes, mas não é possível garatir que não haverá valores faltantes quando o sistema entrar em produção. Então, é seguro aplicar imputer para todos os atributos numéricos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer.statistics_"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Agora, podemos usar esses valores obtidos para substituir valores faltantes pelas medianas aprendidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputer.transform(data_num) # o resultado é um numpy.array. Precisamos converter para DataFrame\n",
    "data_transformed = pd.DataFrame(X, columns=data_num.columns)\n",
    "data_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# podemos confirmar que os valores faltantes foram todos preenchidos\n",
    "data_transformed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulando Atributos Textuais e Categóricos"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Anteriormente, deixamos de lado os atributos categóricos pelo motivo de serem um atributo de texto e, portanto, não é possível calcular sua mediana. Assim, precisamos converter esses rótulos em números. \n",
    "\n",
    "Scikit-Learn provê uma ferramenta para isso denominada OrdinalEncoder, que associa um valor para cada categoria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separando apenas atributos categóricos\n",
    "data_cat = data[['month','day']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "encoder = OrdinalEncoder()\n",
    "data_cat_1hot = encoder.fit_transform(data_cat)\n",
    "data_cat_1hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipelines de Transformação"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Como é possível observar, existem muitos passos no processo de transformação de dados que precisam ser executados na ordem correta. Para isso, Scikit-Learn provê uma classe que auxilia essas sequências de transformações: Pipeline. \n",
    "\n",
    "Para os atributos numéricos, podemos escrever um pipeline da seguinte maneira:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer',SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "])\n",
    "data_num_tr = num_pipeline.fit_transform(data_num)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mas ainda precisamos lidar com os atributos categóricos e, além disso, juntar todo o processo de transformação num único pipeline para obter um único resultado de todos os dados tratados que, em teoria, estariam prontos para um algoritmo de machine learning (obviamente, antes precisariam ser divididos em conjunto de treino e teste - veremos isso adiante). \n",
    "\n",
    "Para criar um pipeline que trate tanto atributos numéricos quanto categóricos, podemos usar FeatureUnion. Antes, no entanto, vamos criar uma classe para separar os atributos numéricos e categóricos de nosso dataset original "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A classe DataFrameSelector irá separar o dataframe de acordo com o conteúdo da variável attribute_names, retornando apenas aquelas que são categórias ou aquelas que são numéricas. Essa classe herda os métodos BaseEstimator e TransformerMixin.\n",
    "\n",
    "O primeiro é responsável por implementar os métodos get_params() e set_params() que são extremamente úteis quando vamos executar um gridsearch, por exemplo. Com essa classe podemos acessar os valores que estão definidos dentro do método __init__, bem como atribuir novos valores apra ele. \n",
    "\n",
    "Já TransformerMixin tras implementado o método fit_transform(), que nada mais é do que aplicar o método fit() e, logo em seguida, o método transform.\n",
    "\n",
    "Esse link pode ajudar a esclarecer melhor as coisas, caso ainda tenham dúvidas: [link](https://github.com/ageron/handson-ml/issues/391)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import OrdinalEncoder #skl 0.20>\n",
    "num_attribs = list(data_num)\n",
    "cat_attribs = ['month','day']\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(num_attribs)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='mean')),\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(cat_attribs)),\n",
    "    ('categorial_encoder', OrdinalEncoder())\n",
    "    ])\n",
    "full_pipeline = FeatureUnion(transformer_list=[\n",
    "    (\"num_pipeline\", num_pipeline),\n",
    "    (\"cat_pipeline\", cat_pipeline),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepared = full_pipeline.fit_transform(data) #retorna um numpy.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora temos nossos dados preparados.\n",
    "data_prepared[0:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
